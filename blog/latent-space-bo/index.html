<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Latent Space Bayesian Optimization | Jagan Shanmugam</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://jagan-shanmugam.github.io/blog/latent-space-bo"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Latent Space Bayesian Optimization | Jagan Shanmugam"><meta data-rh="true" name="description" content="A blog post about Latent Space Bayesian Optimization"><meta data-rh="true" property="og:description" content="A blog post about Latent Space Bayesian Optimization"><meta data-rh="true" property="og:image" content="https://jagan-shanmugam.github.io/img/blog/long-blog-post/cover.jpg"><meta data-rh="true" name="twitter:image" content="https://jagan-shanmugam.github.io/img/blog/long-blog-post/cover.jpg"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2020-03-15T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/jagan-shanmugam"><meta data-rh="true" property="article:tag" content="Bayesian Optimization,Transfer Learning,Thesis"><link data-rh="true" rel="icon" href="https://avatars.githubusercontent.com/u/30863630?v=4"><link data-rh="true" rel="canonical" href="https://jagan-shanmugam.github.io/blog/latent-space-bo"><link data-rh="true" rel="alternate" href="https://jagan-shanmugam.github.io/blog/latent-space-bo" hreflang="en"><link data-rh="true" rel="alternate" href="https://jagan-shanmugam.github.io/blog/latent-space-bo" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://jagan-shanmugam.github.io/blog/latent-space-bo","mainEntityOfPage":"https://jagan-shanmugam.github.io/blog/latent-space-bo","url":"https://jagan-shanmugam.github.io/blog/latent-space-bo","headline":"Latent Space Bayesian Optimization","name":"Latent Space Bayesian Optimization","description":"A blog post about Latent Space Bayesian Optimization","datePublished":"2020-03-15T00:00:00.000Z","author":{"@type":"Person","name":"Jagan Shanmugam","description":"Data Scientist","url":"https://github.com/jagan-shanmugam","image":"https://avatars.githubusercontent.com/u/30863630?v=4"},"image":{"@type":"ImageObject","@id":"https://jagan-shanmugam.github.io/img/blog/long-blog-post/cover.jpg","url":"https://jagan-shanmugam.github.io/img/blog/long-blog-post/cover.jpg","contentUrl":"https://jagan-shanmugam.github.io/img/blog/long-blog-post/cover.jpg","caption":"title image for the blog post: Latent Space Bayesian Optimization"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://jagan-shanmugam.github.io/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Jagan Shanmugam RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Jagan Shanmugam Atom Feed"><link rel="stylesheet" href="/assets/css/styles.cb059e6d.css">
<script src="/assets/js/runtime~main.0461531e.js" defer="defer"></script>
<script src="/assets/js/main.ea1e441a.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="https://avatars.githubusercontent.com/u/30863630?v=4"><div role="region" aria-label="Skip to main content"><a class="skipToContent_dFS7" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="https://avatars.githubusercontent.com/u/30863630?v=4" alt="Jagan Shanmugam Image" class="themedComponent_fqkS themedComponent--light_eI7I"><img src="https://avatars.githubusercontent.com/u/30863630?v=4" alt="Jagan Shanmugam Image" class="themedComponent_fqkS themedComponent--dark_DzHi"></div><b class="navbar__title text--truncate">jagan.sh</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/about">About</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/jagan-shanmugam/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink__cmb"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_WONS colorModeToggle_nVBW"><button class="clean-btn toggleButton_T_PP toggleButtonDisabled_Fm94" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_rqR0"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_Tik4"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_h7ZN"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_Ryae"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_SLDO thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_rbWz margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_zcty">2025</h3><ul class="sidebarItemList_k3OL clean-list"><li class="sidebarItem_uJBt"><a class="sidebarItemLink_c1Wc" href="/blog/MCP-Overview">MCP Overview</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_zcty">2020</h3><ul class="sidebarItemList_k3OL clean-list"><li class="sidebarItem_uJBt"><a class="sidebarItemLink_c1Wc" href="/blog/tictactoe-rl">TicTacToe RL</a></li><li class="sidebarItem_uJBt"><a aria-current="page" class="sidebarItemLink_c1Wc sidebarItemLinkActive_d3f4" href="/blog/latent-space-bo">Latent Space Bayesian Optimization</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_zcty">2019</h3><ul class="sidebarItemList_k3OL clean-list"><li class="sidebarItem_uJBt"><a class="sidebarItemLink_c1Wc" href="/blog/clustering-evolving-data-streams">Clustering Evolving Data Streams</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_ZEzW">Latent Space Bayesian Optimization</h1><div class="container_zs2Y margin-vert--md"><time datetime="2020-03-15T00:00:00.000Z">March 15, 2020</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_GjWm"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/blog/authors/jagan"><img class="avatar__photo authorImage_AROm" src="https://avatars.githubusercontent.com/u/30863630?v=4" alt="Jagan Shanmugam"></a><div class="avatar__intro authorDetails_DL5X"><div class="avatar__name"><a href="/blog/authors/jagan"><span class="authorName_HbtT">Jagan Shanmugam</span></a></div><small class="authorTitle_f6wt" title="Data Scientist">Data Scientist</small><div class="authorSocials_oWk6"><a href="https://x.com/EeraVengayam" target="_blank" rel="noopener noreferrer" class="authorSocialLink_Y9U6" title="X"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="none" viewBox="0 0 1200 1227" style="--dark:#000;--light:#fff" class="authorSocialLink_Y9U6 xSvg_ikBg"><path d="M714.163 519.284 1160.89 0h-105.86L667.137 450.887 357.328 0H0l468.492 681.821L0 1226.37h105.866l409.625-476.152 327.181 476.152H1200L714.137 519.284h.026ZM569.165 687.828l-47.468-67.894-377.686-540.24h162.604l304.797 435.991 47.468 67.894 396.2 566.721H892.476L569.165 687.854v-.026Z"></path></svg></a><a href="https://github.com/jagan-shanmugam" target="_blank" rel="noopener noreferrer" class="authorSocialLink_Y9U6" title="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 256 250" preserveAspectRatio="xMidYMid" style="--dark:#000;--light:#fff" class="authorSocialLink_Y9U6 githubSvg_AxoN"><path d="M128.001 0C57.317 0 0 57.307 0 128.001c0 56.554 36.676 104.535 87.535 121.46 6.397 1.185 8.746-2.777 8.746-6.158 0-3.052-.12-13.135-.174-23.83-35.61 7.742-43.124-15.103-43.124-15.103-5.823-14.795-14.213-18.73-14.213-18.73-11.613-7.944.876-7.78.876-7.78 12.853.902 19.621 13.19 19.621 13.19 11.417 19.568 29.945 13.911 37.249 10.64 1.149-8.272 4.466-13.92 8.127-17.116-28.431-3.236-58.318-14.212-58.318-63.258 0-13.975 5-25.394 13.188-34.358-1.329-3.224-5.71-16.242 1.24-33.874 0 0 10.749-3.44 35.21 13.121 10.21-2.836 21.16-4.258 32.038-4.307 10.878.049 21.837 1.47 32.066 4.307 24.431-16.56 35.165-13.12 35.165-13.12 6.967 17.63 2.584 30.65 1.255 33.873 8.207 8.964 13.173 20.383 13.173 34.358 0 49.163-29.944 59.988-58.447 63.157 4.591 3.972 8.682 11.762 8.682 23.704 0 17.126-.148 30.91-.148 35.126 0 3.407 2.304 7.398 8.792 6.14C219.37 232.5 256 184.537 256 128.002 256 57.307 198.691 0 128.001 0Zm-80.06 182.34c-.282.636-1.283.827-2.194.39-.929-.417-1.45-1.284-1.15-1.922.276-.655 1.279-.838 2.205-.399.93.418 1.46 1.293 1.139 1.931Zm6.296 5.618c-.61.566-1.804.303-2.614-.591-.837-.892-.994-2.086-.375-2.66.63-.566 1.787-.301 2.626.591.838.903 1 2.088.363 2.66Zm4.32 7.188c-.785.545-2.067.034-2.86-1.104-.784-1.138-.784-2.503.017-3.05.795-.547 2.058-.055 2.861 1.075.782 1.157.782 2.522-.019 3.08Zm7.304 8.325c-.701.774-2.196.566-3.29-.49-1.119-1.032-1.43-2.496-.726-3.27.71-.776 2.213-.558 3.315.49 1.11 1.03 1.45 2.505.701 3.27Zm9.442 2.81c-.31 1.003-1.75 1.459-3.199 1.033-1.448-.439-2.395-1.613-2.103-2.626.301-1.01 1.747-1.484 3.207-1.028 1.446.436 2.396 1.602 2.095 2.622Zm10.744 1.193c.036 1.055-1.193 1.93-2.715 1.95-1.53.034-2.769-.82-2.786-1.86 0-1.065 1.202-1.932 2.733-1.958 1.522-.03 2.768.818 2.768 1.868Zm10.555-.405c.182 1.03-.875 2.088-2.387 2.37-1.485.271-2.861-.365-3.05-1.386-.184-1.056.893-2.114 2.376-2.387 1.514-.263 2.868.356 3.061 1.403Z"></path></svg></a><a href="https://www.linkedin.com/in/jaganshanmugam/" target="_blank" rel="noopener noreferrer" class="authorSocialLink_Y9U6" title="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" preserveAspectRatio="xMidYMid" viewBox="0 0 256 256" class="authorSocialLink_Y9U6"><path d="M218.123 218.127h-37.931v-59.403c0-14.165-.253-32.4-19.728-32.4-19.756 0-22.779 15.434-22.779 31.369v60.43h-37.93V95.967h36.413v16.694h.51a39.907 39.907 0 0 1 35.928-19.733c38.445 0 45.533 25.288 45.533 58.186l-.016 67.013ZM56.955 79.27c-12.157.002-22.014-9.852-22.016-22.009-.002-12.157 9.851-22.014 22.008-22.016 12.157-.003 22.014 9.851 22.016 22.008A22.013 22.013 0 0 1 56.955 79.27m18.966 138.858H37.95V95.967h37.97v122.16ZM237.033.018H18.89C8.58-.098.125 8.161-.001 18.471v219.053c.122 10.315 8.576 18.582 18.89 18.474h218.144c10.336.128 18.823-8.139 18.966-18.474V18.454c-.147-10.33-8.635-18.588-18.966-18.453" fill="#0A66C2"></path></svg></a></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p>Optimization is everywhere - in tuning machine learning models, industrial processes, and even in everyday decision-making. But what happens when the problem you want to optimize is a black box, expensive to evaluate, and has way too many parameters? That&#x27;s where my master&#x27;s thesis comes in: Latent Space Bayesian Optimization with Transfer Learning. Here&#x27;s a deep dive into what I did, why it matters, and what I learned along the way.</p>
<h2 class="anchor anchorWithStickyNavbar_QNvs" id="the-problem-black-box-optimization-in-high-dimensions">The Problem: Black-Box Optimization in High Dimensions<a href="#the-problem-black-box-optimization-in-high-dimensions" class="hash-link" aria-label="Direct link to The Problem: Black-Box Optimization in High Dimensions" title="Direct link to The Problem: Black-Box Optimization in High Dimensions">​</a></h2>
<p>Let&#x27;s set the stage. Imagine you&#x27;re trying to optimize a process - say, welding parameters in manufacturing. You have a ton of parameters (temperature, speed, material type, etc.), but only a few of them really matter for the final outcome. The catch? Every experiment is expensive, noisy, and you don&#x27;t have an explicit formula to optimize. This is a classic expensive black-box optimization problem, and Bayesian Optimization (BO) is a popular tool for it.</p>
<p>But BO has a weakness: it doesn&#x27;t scale well with high-dimensional spaces. Most of the regions in high-dimensional parameter space are flat, making it hard to find the optimum quickly. Plus, if you&#x27;ve optimized similar problems before, you&#x27;d want to use that experience to speed things up - that&#x27;s where transfer learning comes in.</p>
<h2 class="anchor anchorWithStickyNavbar_QNvs" id="the-core-idea-optimize-where-it-matters">The Core Idea: Optimize Where It Matters<a href="#the-core-idea-optimize-where-it-matters" class="hash-link" aria-label="Direct link to The Core Idea: Optimize Where It Matters" title="Direct link to The Core Idea: Optimize Where It Matters">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_QNvs" id="latent-spaces">Latent Spaces<a href="#latent-spaces" class="hash-link" aria-label="Direct link to Latent Spaces" title="Direct link to Latent Spaces">​</a></h3>
<p>The key insight is that, even though your input space might be huge (dozens or hundreds of parameters), the intrinsic dimensionality is often much lower. In other words, only a few directions in parameter space actually affect your objective. If you can learn a transformation from the high-dimensional input to a low-dimensional latent space that captures the important variation, you can optimize much more efficiently.</p>
<h3 class="anchor anchorWithStickyNavbar_QNvs" id="transfer-learning">Transfer Learning<a href="#transfer-learning" class="hash-link" aria-label="Direct link to Transfer Learning" title="Direct link to Transfer Learning">​</a></h3>
<p>If you&#x27;ve already solved similar optimization problems (maybe with different materials or settings), you should be able to transfer what you&#x27;ve learned. The challenge is to design a model that can leverage this metadata (past optimization runs) to &quot;warm start&quot; the new optimization, avoiding the cold start problem that plagues standard BO.</p>
<h2 class="anchor anchorWithStickyNavbar_QNvs" id="my-approach-joint-learning-in-latent-space">My Approach: Joint Learning in Latent Space<a href="#my-approach-joint-learning-in-latent-space" class="hash-link" aria-label="Direct link to My Approach: Joint Learning in Latent Space" title="Direct link to My Approach: Joint Learning in Latent Space">​</a></h2>
<p>My thesis proposes a method that jointly learns:</p>
<ul>
<li>A transformation from input space to latent space (either linear or nonlinear)</li>
<li>A shared set of features (basis functions) across multiple tasks for transfer learning</li>
</ul>
<p>The model is trained in two phases:</p>
<ol>
<li><strong>Meta-training:</strong> Learn from metadata (previous tasks) to initialize the latent space and shared features.</li>
<li><strong>Target training:</strong> Adapt the model to the new task, fine-tuning both the latent space and the prediction model as new data comes in.</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_QNvs" id="model-architecture">Model Architecture<a href="#model-architecture" class="hash-link" aria-label="Direct link to Model Architecture" title="Direct link to Model Architecture">​</a></h2>
<p>I explored two main variants:</p>
<ul>
<li><strong>Projection-ABLR:</strong> Uses a learnable linear projection from input to latent space, paired with Adaptive Bayesian Linear Regression (ABLR) for prediction.</li>
<li><strong>AutoEncoder-ABLR:</strong> Uses an autoencoder (neural network) to learn a nonlinear mapping to latent space, again paired with ABLR.</li>
</ul>
<p>Both models are trained to minimize a combination of negative log-likelihood (for prediction) and mean squared error (for reconstructing the input from the latent space). This joint loss ensures the latent space is both predictive and reconstructive.</p>
<h3 class="anchor anchorWithStickyNavbar_QNvs" id="why-ablr">Why ABLR?<a href="#why-ablr" class="hash-link" aria-label="Direct link to Why ABLR?" title="Direct link to Why ABLR?">​</a></h3>
<p>Adaptive Bayesian Linear Regression is computationally efficient and scales well with the number of tasks and data points - crucial for transfer learning. It allows for a separate Bayesian regressor for each task, but shares the feature mapping, making it ideal for multi-task scenarios.</p>
<h2 class="anchor anchorWithStickyNavbar_QNvs" id="experiments-synthetic-benchmarks">Experiments: Synthetic Benchmarks<a href="#experiments-synthetic-benchmarks" class="hash-link" aria-label="Direct link to Experiments: Synthetic Benchmarks" title="Direct link to Experiments: Synthetic Benchmarks">​</a></h2>
<p>To test the method, I used high-dimensional synthetic functions with known low intrinsic dimensionality:</p>
<ul>
<li><strong>Quadratic function:</strong> Parameterized by a small set of variables, projected into higher dimensions.</li>
<li><strong>Rosenbrock function:</strong> A classic optimization benchmark, adapted for multi-task and high-dimensional settings.</li>
</ul>
<p>I compared my models against state-of-the-art baselines:</p>
<ul>
<li><strong>REMBO:</strong> Random Embeddings for Bayesian Optimization (no transfer learning)</li>
<li><strong>Multi-Task ABLR:</strong> Directly models in the input space with transfer learning</li>
<li><strong>VAE-BO:</strong> Variational Autoencoder-based Bayesian Optimization</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_QNvs" id="key-results">Key Results<a href="#key-results" class="hash-link" aria-label="Direct link to Key Results" title="Direct link to Key Results">​</a></h2>
<ul>
<li><strong>Transfer learning helps:</strong> Models that leverage metadata start with lower regret (closer to the optimum) and converge faster, especially in the early iterations.</li>
<li><strong>Latent space optimization is efficient:</strong> By optimizing in the learned low-dimensional space, the search is much more effective than in the original high-dimensional space.</li>
<li><strong>Projection-ABLR outperforms:</strong> The linear projection model consistently achieved lower regret than baselines, especially when enough metadata was available.</li>
<li><strong>AutoEncoder-ABLR needs more data:</strong> Nonlinear models (autoencoders) can capture more complex relationships but require more data to avoid overfitting or saturation.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_QNvs" id="challenges-and-open-questions">Challenges and Open Questions<a href="#challenges-and-open-questions" class="hash-link" aria-label="Direct link to Challenges and Open Questions" title="Direct link to Challenges and Open Questions">​</a></h2>
<ul>
<li><strong>Estimating intrinsic dimensionality:</strong> Knowing how many latent dimensions to use is still an open problem. I tried cross-validation and meta-loss analysis, but the results were inconclusive. This remains a key challenge for future work.</li>
<li><strong>Scaling to real-world tasks:</strong> The method works well on synthetic benchmarks. Applying it to real industrial processes (like welding) is the next step.</li>
<li><strong>Negative transfer:</strong> If the metadata tasks are too different from the target, transfer learning can actually hurt performance. Designing robust ways to detect and avoid negative transfer is important.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_QNvs" id="takeaways">Takeaways<a href="#takeaways" class="hash-link" aria-label="Direct link to Takeaways" title="Direct link to Takeaways">​</a></h2>
<ul>
<li><strong>Joint learning works:</strong> Simultaneously learning the latent space and prediction model is more effective than sequential approaches, especially when data is scarce.</li>
<li><strong>Transfer learning is powerful:</strong> Leveraging past experience can dramatically speed up optimization in new tasks.</li>
<li><strong>Linear vs. nonlinear:</strong> Linear projections are surprisingly effective when the intrinsic structure is simple, but nonlinear mappings (autoencoders) are more flexible for complex tasks - if you have enough data.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_QNvs" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>My thesis shows that Latent Space Bayesian Optimization with Transfer Learning is a promising approach for high-dimensional, expensive black-box optimization. By learning where to search (latent space) and how to transfer knowledge from previous tasks, we can solve challenging optimization problems more efficiently.</p>
<p>If you&#x27;re working on hyperparameter tuning, industrial process optimization, or any scenario where experiments are costly and you have some prior data, this approach could save you time, money, and frustration.</p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_NT0z padding--none margin-left--sm"><li class="tag_BkA8"><a class="tag_kkj1 tagRegular_kyfp" href="/blog/tags/bayesian-optimization">Bayesian Optimization</a></li><li class="tag_BkA8"><a class="tag_kkj1 tagRegular_kyfp" href="/blog/tags/transfer-learning">Transfer Learning</a></li><li class="tag_BkA8"><a class="tag_kkj1 tagRegular_kyfp" href="/blog/tags/thesis">Thesis</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><a href="https://github.com/jagan-shanmugam/jagan-shanmugam.github.io/tree/main/blog/2020-03-15-latent-space-bo/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_mWhq" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_XN5G"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/tictactoe-rl"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">TicTacToe RL</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/clustering-evolving-data-streams"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Clustering Evolving Data Streams</div></a></nav></main><div class="col col--2"><div class="tableOfContents_fNFI thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-problem-black-box-optimization-in-high-dimensions" class="table-of-contents__link toc-highlight">The Problem: Black-Box Optimization in High Dimensions</a></li><li><a href="#the-core-idea-optimize-where-it-matters" class="table-of-contents__link toc-highlight">The Core Idea: Optimize Where It Matters</a><ul><li><a href="#latent-spaces" class="table-of-contents__link toc-highlight">Latent Spaces</a></li><li><a href="#transfer-learning" class="table-of-contents__link toc-highlight">Transfer Learning</a></li></ul></li><li><a href="#my-approach-joint-learning-in-latent-space" class="table-of-contents__link toc-highlight">My Approach: Joint Learning in Latent Space</a></li><li><a href="#model-architecture" class="table-of-contents__link toc-highlight">Model Architecture</a><ul><li><a href="#why-ablr" class="table-of-contents__link toc-highlight">Why ABLR?</a></li></ul></li><li><a href="#experiments-synthetic-benchmarks" class="table-of-contents__link toc-highlight">Experiments: Synthetic Benchmarks</a></li><li><a href="#key-results" class="table-of-contents__link toc-highlight">Key Results</a></li><li><a href="#challenges-and-open-questions" class="table-of-contents__link toc-highlight">Challenges and Open Questions</a></li><li><a href="#takeaways" class="table-of-contents__link toc-highlight">Takeaways</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">About</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/about">CV</a></li></ul></div><div class="col footer__col"><div class="footer__title">Socials</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.linkedin.com/in/jaganshanmugam/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink__cmb"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://x.com/EeraVengayam" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink__cmb"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://bsky.app/profile/jagans.bsky.social" target="_blank" rel="noopener noreferrer" class="footer__link-item">BlueSky<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink__cmb"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/jagan-shanmugam/" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink__cmb"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Jagan Shanmugam. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>