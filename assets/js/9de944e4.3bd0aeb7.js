"use strict";(globalThis.webpackChunkjagan_sh=globalThis.webpackChunkjagan_sh||[]).push([[9353],{2271(e){e.exports=JSON.parse('{"permalink":"/blog/latent-space-bo","editUrl":"https://github.com/jagan-shanmugam/jagan-shanmugam.github.io/tree/main/blog/2020-03-15-latent-space-bo/index.md","source":"@site/blog/2020-03-15-latent-space-bo/index.md","title":"Latent Space Bayesian Optimization","description":"A blog post about Latent Space Bayesian Optimization","date":"2020-03-15T00:00:00.000Z","tags":[{"inline":false,"label":"Bayesian Optimization","permalink":"/blog/tags/bayesian-optimization"},{"inline":false,"label":"Transfer Learning","permalink":"/blog/tags/transfer-learning"},{"inline":false,"label":"Thesis","permalink":"/blog/tags/thesis"}],"readingTime":4.85,"hasTruncateMarker":true,"authors":[{"name":"Jagan Shanmugam","title":"Data Scientist","url":"https://github.com/jagan-shanmugam","page":{"permalink":"/blog/authors/jagan"},"socials":{"x":"https://x.com/EeraVengayam","github":"https://github.com/jagan-shanmugam","linkedin":"https://www.linkedin.com/in/jaganshanmugam/"},"imageURL":"https://avatars.githubusercontent.com/u/30863630?v=4","key":"jagan"}],"frontMatter":{"slug":"latent-space-bo","title":"Latent Space Bayesian Optimization","image":"/img/blog/long-blog-post/cover.jpg","description":"A blog post about Latent Space Bayesian Optimization","authors":"jagan","tags":["bayesian-optimization","transfer-learning","thesis"]},"unlisted":false,"prevItem":{"title":"TicTacToe RL","permalink":"/blog/tictactoe-rl"},"nextItem":{"title":"Clustering Evolving Data Streams","permalink":"/blog/clustering-evolving-data-streams"}}')},3434(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>a,toc:()=>h});var a=i(2271),t=i(4848),s=i(8453);const r={slug:"latent-space-bo",title:"Latent Space Bayesian Optimization",image:"/img/blog/long-blog-post/cover.jpg",description:"A blog post about Latent Space Bayesian Optimization",authors:"jagan",tags:["bayesian-optimization","transfer-learning","thesis"]},o="Latent Space Bayesian Optimization with Transfer Learning: My Thesis Journey",l={authorsImageUrls:[void 0]},h=[{value:"The Problem: Black-Box Optimization in High Dimensions",id:"the-problem-black-box-optimization-in-high-dimensions",level:2},{value:"The Core Idea: Optimize Where It Matters",id:"the-core-idea-optimize-where-it-matters",level:2},{value:"Latent Spaces",id:"latent-spaces",level:3},{value:"Transfer Learning",id:"transfer-learning",level:3},{value:"My Approach: Joint Learning in Latent Space",id:"my-approach-joint-learning-in-latent-space",level:2},{value:"Model Architecture",id:"model-architecture",level:2},{value:"Why ABLR?",id:"why-ablr",level:3},{value:"Experiments: Synthetic Benchmarks",id:"experiments-synthetic-benchmarks",level:2},{value:"Key Results",id:"key-results",level:2},{value:"Challenges and Open Questions",id:"challenges-and-open-questions",level:2},{value:"Takeaways",id:"takeaways",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const n={h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"Optimization is everywhere - in tuning machine learning models, industrial processes, and even in everyday decision-making. But what happens when the problem you want to optimize is a black box, expensive to evaluate, and has way too many parameters? That's where my master's thesis comes in: Latent Space Bayesian Optimization with Transfer Learning. Here's a deep dive into what I did, why it matters, and what I learned along the way."}),"\n",(0,t.jsx)(n.h2,{id:"the-problem-black-box-optimization-in-high-dimensions",children:"The Problem: Black-Box Optimization in High Dimensions"}),"\n",(0,t.jsx)(n.p,{children:"Let's set the stage. Imagine you're trying to optimize a process - say, welding parameters in manufacturing. You have a ton of parameters (temperature, speed, material type, etc.), but only a few of them really matter for the final outcome. The catch? Every experiment is expensive, noisy, and you don't have an explicit formula to optimize. This is a classic expensive black-box optimization problem, and Bayesian Optimization (BO) is a popular tool for it."}),"\n",(0,t.jsx)(n.p,{children:"But BO has a weakness: it doesn't scale well with high-dimensional spaces. Most of the regions in high-dimensional parameter space are flat, making it hard to find the optimum quickly. Plus, if you've optimized similar problems before, you'd want to use that experience to speed things up - that's where transfer learning comes in."}),"\n",(0,t.jsx)(n.h2,{id:"the-core-idea-optimize-where-it-matters",children:"The Core Idea: Optimize Where It Matters"}),"\n",(0,t.jsx)(n.h3,{id:"latent-spaces",children:"Latent Spaces"}),"\n",(0,t.jsx)(n.p,{children:"The key insight is that, even though your input space might be huge (dozens or hundreds of parameters), the intrinsic dimensionality is often much lower. In other words, only a few directions in parameter space actually affect your objective. If you can learn a transformation from the high-dimensional input to a low-dimensional latent space that captures the important variation, you can optimize much more efficiently."}),"\n",(0,t.jsx)(n.h3,{id:"transfer-learning",children:"Transfer Learning"}),"\n",(0,t.jsx)(n.p,{children:"If you've already solved similar optimization problems (maybe with different materials or settings), you should be able to transfer what you've learned. The challenge is to design a model that can leverage this metadata (past optimization runs) to \"warm start\" the new optimization, avoiding the cold start problem that plagues standard BO."}),"\n",(0,t.jsx)(n.h2,{id:"my-approach-joint-learning-in-latent-space",children:"My Approach: Joint Learning in Latent Space"}),"\n",(0,t.jsx)(n.p,{children:"My thesis proposes a method that jointly learns:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A transformation from input space to latent space (either linear or nonlinear)"}),"\n",(0,t.jsx)(n.li,{children:"A shared set of features (basis functions) across multiple tasks for transfer learning"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The model is trained in two phases:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Meta-training:"})," Learn from metadata (previous tasks) to initialize the latent space and shared features."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Target training:"})," Adapt the model to the new task, fine-tuning both the latent space and the prediction model as new data comes in."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,t.jsx)(n.p,{children:"I explored two main variants:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Projection-ABLR:"})," Uses a learnable linear projection from input to latent space, paired with Adaptive Bayesian Linear Regression (ABLR) for prediction."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"AutoEncoder-ABLR:"})," Uses an autoencoder (neural network) to learn a nonlinear mapping to latent space, again paired with ABLR."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Both models are trained to minimize a combination of negative log-likelihood (for prediction) and mean squared error (for reconstructing the input from the latent space). This joint loss ensures the latent space is both predictive and reconstructive."}),"\n",(0,t.jsx)(n.h3,{id:"why-ablr",children:"Why ABLR?"}),"\n",(0,t.jsx)(n.p,{children:"Adaptive Bayesian Linear Regression is computationally efficient and scales well with the number of tasks and data points - crucial for transfer learning. It allows for a separate Bayesian regressor for each task, but shares the feature mapping, making it ideal for multi-task scenarios."}),"\n",(0,t.jsx)(n.h2,{id:"experiments-synthetic-benchmarks",children:"Experiments: Synthetic Benchmarks"}),"\n",(0,t.jsx)(n.p,{children:"To test the method, I used high-dimensional synthetic functions with known low intrinsic dimensionality:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Quadratic function:"})," Parameterized by a small set of variables, projected into higher dimensions."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Rosenbrock function:"})," A classic optimization benchmark, adapted for multi-task and high-dimensional settings."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"I compared my models against state-of-the-art baselines:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"REMBO:"})," Random Embeddings for Bayesian Optimization (no transfer learning)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Task ABLR:"})," Directly models in the input space with transfer learning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"VAE-BO:"})," Variational Autoencoder-based Bayesian Optimization"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"key-results",children:"Key Results"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transfer learning helps:"})," Models that leverage metadata start with lower regret (closer to the optimum) and converge faster, especially in the early iterations."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Latent space optimization is efficient:"})," By optimizing in the learned low-dimensional space, the search is much more effective than in the original high-dimensional space."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Projection-ABLR outperforms:"})," The linear projection model consistently achieved lower regret than baselines, especially when enough metadata was available."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"AutoEncoder-ABLR needs more data:"})," Nonlinear models (autoencoders) can capture more complex relationships but require more data to avoid overfitting or saturation."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"challenges-and-open-questions",children:"Challenges and Open Questions"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Estimating intrinsic dimensionality:"})," Knowing how many latent dimensions to use is still an open problem. I tried cross-validation and meta-loss analysis, but the results were inconclusive. This remains a key challenge for future work."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scaling to real-world tasks:"})," The method works well on synthetic benchmarks. Applying it to real industrial processes (like welding) is the next step."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Negative transfer:"})," If the metadata tasks are too different from the target, transfer learning can actually hurt performance. Designing robust ways to detect and avoid negative transfer is important."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"takeaways",children:"Takeaways"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Joint learning works:"})," Simultaneously learning the latent space and prediction model is more effective than sequential approaches, especially when data is scarce."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transfer learning is powerful:"})," Leveraging past experience can dramatically speed up optimization in new tasks."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Linear vs. nonlinear:"})," Linear projections are surprisingly effective when the intrinsic structure is simple, but nonlinear mappings (autoencoders) are more flexible for complex tasks - if you have enough data."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"My thesis shows that Latent Space Bayesian Optimization with Transfer Learning is a promising approach for high-dimensional, expensive black-box optimization. By learning where to search (latent space) and how to transfer knowledge from previous tasks, we can solve challenging optimization problems more efficiently."}),"\n",(0,t.jsx)(n.p,{children:"If you're working on hyperparameter tuning, industrial process optimization, or any scenario where experiments are costly and you have some prior data, this approach could save you time, money, and frustration."})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>o});var a=i(6540);const t={},s=a.createContext(t);function r(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);