"use strict";(self.webpackChunkjagan_sh=self.webpackChunkjagan_sh||[]).push([[2614],{8462:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>a,toc:()=>o});var a=i(8851),t=i(3420),r=i(720);const s={slug:"tictactoe-rl",title:"TicTacToe RL",authors:["jagan"],tags:[]},l="Tic Tac Toe with Reinforcement Learning",c={authorsImageUrls:[void 0]},o=[{value:"Project Overview",id:"project-overview",level:2},{value:"How It Works",id:"how-it-works",level:2},{value:"Directory Structure",id:"directory-structure",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Gameplay",id:"gameplay",level:2},{value:"References",id:"references",level:2}];function h(e){const n={a:"a",code:"code",h2:"h2",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"This is an implementation of the classic Tic Tac Toe game, powered by Reinforcement Learning (RL)! This project demonstrates how an RL agent can learn to play Tic Tac Toe optimally through self-play and Temporal Difference (TD) learning."}),"\n",(0,t.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Purpose:"})," Train an RL agent to play Tic Tac Toe using self-play and TD(0) learning, and provide both a command-line and graphical interface for users to play against the trained agent."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Key Features:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"RL agent learns state values for all possible board configurations (over 19,000 states)"}),"\n",(0,t.jsx)(n.li,{children:"Epsilon-greedy policy for balancing exploration and exploitation during training"}),"\n",(0,t.jsx)(n.li,{children:"Pygame-based graphical UI for interactive play"}),"\n",(0,t.jsx)(n.li,{children:"Command-line interface for quick testing"}),"\n",(0,t.jsx)(n.li,{children:"Well-documented code and modular structure"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"how-it-works",children:"How It Works"}),"\n",(0,t.jsx)(n.p,{children:"The RL agent is trained using a simple TD(0) update rule:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"v(s) \u2190 v(s) + \u03b1 (v(s') - v(s))\n"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"v(s):"})," Value of the current state"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"v(s'):"})," Value of the next state"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\u03b1:"})," Learning rate"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"During training, the agent plays games against itself, updating state values based on the outcome and gradually improving its strategy. The agent uses an epsilon-greedy policy to occasionally explore random moves, ensuring a robust learning process."}),"\n",(0,t.jsx)(n.h2,{id:"directory-structure",children:"Directory Structure"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"game_app.py:"})," Pygame-based UI for playing against the RL agent"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"test_game.py:"})," Command-line interface for testing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"training_self_play.py:"})," RL training logic"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"tic_tac_toe.py:"})," Core game logic and state management"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"requirements:"})," Python dependencies"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Install Dependencies:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Python 3.5+"}),"\n",(0,t.jsxs)(n.li,{children:["Install required packages:","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"pip install -r requirements\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Train the RL Agent:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Run ",(0,t.jsx)(n.code,{children:"training_self_play.py"})," to train the agent (optional, pre-trained values included)"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Play the Game:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Graphical UI:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"python game_app.py\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Command-line:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"python test_game.py\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"gameplay",children:"Gameplay"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The RL agent can play as either X or O."}),"\n",(0,t.jsx)(n.li,{children:"In the Pygame UI, the agent and user take turns; click on a square to make your move."}),"\n",(0,t.jsx)(n.li,{children:"After each game, click anywhere to restart."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/jagan-shanmugam/TicTacToe-RL",children:"Tic Tac Toe RL GitHub Repository"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://ipvs.informatik.uni-stuttgart.de/mlr/wp-content/uploads/2018/05/18-RL-td.pdf",children:"Temporal Difference Learning (PDF)"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://nestedsoftware.com/2019/07/25/tic-tac-toe-with-tabular-q-learning-1kdn.139811.html",children:"Q-Learning Tic Tac Toe Tutorial"})}),"\n"]}),"\n",(0,t.jsx)(n.hr,{})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},8851:e=>{e.exports=JSON.parse('{"permalink":"/blog/tictactoe-rl","editUrl":"https://github.com/jagan-shanmugam/jagan-shanmugam.github.io/tree/main/blog/2020-04-20-TicTacToe RL/index.md","source":"@site/blog/2020-04-20-TicTacToe RL/index.md","title":"TicTacToe RL","description":"This is an implementation of the classic Tic Tac Toe game, powered by Reinforcement Learning (RL)! This project demonstrates how an RL agent can learn to play Tic Tac Toe optimally through self-play and Temporal Difference (TD) learning.","date":"2020-04-20T00:00:00.000Z","tags":[],"readingTime":1.75,"hasTruncateMarker":true,"authors":[{"name":"Jagan Shanmugam","title":"Data Scientist","url":"https://github.com/jagan-shanmugam","page":{"permalink":"/blog/authors/jagan"},"socials":{"x":"https://x.com/EeraVengayam","github":"https://github.com/jagan-shanmugam","linkedin":"https://www.linkedin.com/in/jaganshanmugam/"},"imageURL":"https://avatars.githubusercontent.com/u/30863630?v=4","key":"jagan"}],"frontMatter":{"slug":"tictactoe-rl","title":"TicTacToe RL","authors":["jagan"],"tags":[]},"unlisted":false,"prevItem":{"title":"MCP Overview","permalink":"/blog/MCP-Overview"},"nextItem":{"title":"Latent Space Bayesian Optimization","permalink":"/blog/latent-space-bo"}}')}}]);